{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我的飞行器需要训练多久？\n",
    "根据随机的初始参数，有时你的飞行器会在初始的20-50个迭代循环里习得你的任务，但有时你会可能需要这个训练次数达到500-1000次。此外，你的飞行器也可能陷入局部最优解，而无法达到更优的学习效果。因此，你的飞行器可能需要很长时间来完成你的任务，这个主要依赖于你选择的学习率learning rate。\n",
    "\n",
    "如果你发现你的飞行器学习了1500-2000次仍然没有显著进展，你可以先让你的飞行器先试试简单的连续动作问题，如 [Mountain Car](https://gym.openai.com/envs/MountainCarContinuous-v0/) 或者 [Pendulum](https://gym.openai.com/envs/Pendulum-v0/) 。当然，你的模型大小会随着你状态/行动空间的不同定义而改变，但是至少可以保障你的算法实现。\n",
    "\n",
    "其次，调整飞行器的状态以及/或者动作空间定义，来保障你的飞行器拥有足够的学习参数（但不要过多！）最后（这是许多最多迭代的部分），尝试调整奖励函数来让你的飞行器得到一个更具体的学习目的。调整矩阵里的权重，增加额外的奖励/惩罚，尝试一个非线性映射矩阵。同时，你可能也需要将奖惩的值标准化，如从-1.0到1.0。这样的转换将避免梯度爆炸造成的不稳定性。\n",
    "\n",
    "## 我的飞行器的确在学习，但是即便是过了很久，依然没有达到我的要求。我该怎么做？\n",
    "想要让一个强化学习智能体学习你_实际_想要它学的，是十分困难的，也十分费时。你的飞行器只会根据你定义的奖励函数来习得最优的策略，但是你的奖励函数可能没有包含你期待的所有方面，比如飞行任务等。如果你没有预设你的飞行器做一些转体，只是关心它飞多高，那它就很可能会旋转上升！\n",
    "\n",
    "有时，你的算法可能没有与环境交互好，如，可能部分环境需要额外的探索及噪音，而部分环境则可以减少噪音。你可以试试改变一些变量，来看会不会优化结果。但是即使你已经有了一个很优秀的算法，你都可能需要几天或几周来习得一个复杂的任务！\n",
    "\n",
    "这就是为什么我们希望你绘制一个奖励曲线。**只要每次迭代的平均奖励是总体上升的（即便存在一些噪音），就可以了。飞行最后的行为展示不需要非常完美。**\n",
    "\n",
    "## 我可以使用一个深度Q网络来解决问题吗？\n",
    "当然！但是...记得一个深度Q网络（DQN）意味着解决一个离散动作空间问题，然而我们的飞行器控制问题是一个连续动作域问题。因此我们并不建议一个离散空间算法，但是你可以仍旧使用这个DQN来映射最终的输出结果至一个适当的连续动作空间。尽管这个过程可能让神经网络难以理解动作之间的互相关联，但是它可能也会简化这个训练算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
