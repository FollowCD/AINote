{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG: 智能体\n",
    "现在我们可以将行动者模型和策略模型放到一起，构建 DDPG 智能体了。注意，我们需要每个模型的两个副本，一个本地副本，一个目标副本。该技巧来自深度 Q 学习中的“固定 Q 目标”，用于拆分被更新的参数和生成目标值的参数。\n",
    "\n",
    "以下是智能体类的纲要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        self.state_size = task.state_size\n",
    "        self.action_size = task.action_size\n",
    "        self.action_low = task.action_low\n",
    "        self.action_high = task.action_high\n",
    "\n",
    "        # Actor (Policy) Model\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "\n",
    "        # Critic (Value) Model\n",
    "        self.critic_local = Critic(self.state_size, self.action_size)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size)\n",
    "\n",
    "        # Initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        # Noise process\n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        self.exploration_sigma = 0.2\n",
    "        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n",
    "\n",
    "        # Replay memory\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.tau = 0.01  # for soft update of target parameters\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "        state = self.task.reset()\n",
    "        self.last_state = state\n",
    "        return state\n",
    "\n",
    "    def step(self, action, reward, next_state, done):\n",
    "         # Save experience / reward\n",
    "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "        # Roll over last state and action\n",
    "        self.last_state = next_state\n",
    "\n",
    "    def act(self, states):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        action = self.actor_local.model.predict(state)[0]\n",
    "        return list(action + self.noise.sample())  # add some noise for exploration\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        #     Q_targets_next = critic_target(next_state, actor_target(next_state))\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "\n",
    "        # Compute Q targets for current states and train critic model (local)\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        # Train actor model (local)\n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n",
    "\n",
    "        # Soft-update target models\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)   \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights), \"Local and target model parameters must have the same size\"\n",
    "\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在用多个经验进行训练后，我们可以将新学习的权重（来自本地模型）复制到目标模型中。但是，单个批次可能会向该流程中引入很多偏差，因此最后进行软更新，由参数 tau 控制。\n",
    "\n",
    "最后还要完成一步，才能使整个项目能正常运行，你需要创建合适的噪点模型（将在下部分讲解）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
